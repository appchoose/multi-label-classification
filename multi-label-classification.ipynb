{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eren/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from sklearn.manifold import TSNE\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras import backend as K\n",
    "from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, \\\n",
    "    GlobalMaxPool2D, Concatenate, GlobalMaxPooling2D, GlobalAveragePooling2D, Lambda\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import functools\n",
    "import cv2 as cv\n",
    "%pylab inline\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import urllib\n",
    "from PIL import Image\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 256\n",
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 100\n",
    "steps_per_epoch = 100\n",
    "INIT_LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels)\n",
    "print (labels[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "lbl = mlb.fit_transform(labels)\n",
    "lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (mlb.classes_)\n",
    "print (len(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(imgs,\n",
    "                                                      lbl,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=[150, 150, 3])\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "# you need to use sigmoid instead of softmax for multi-label classification\n",
    "# https://stackoverflow.com/questions/44164749/how-does-keras-handle-multilabel-classification?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "predictions = Dense(len(mlb.classes_), activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "print(\"Number of layers : \" + str(len(model.layers)))\n",
    "\n",
    "multi_model = multi_gpu_model(model, gpus = 2)\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "for layer in model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer = 'Adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['categorical_accuracy'])\n",
    "\n",
    "#if yopu just want to train on a single gpu, just comment the multi_model\n",
    "multi_model.compile(optimizer = 'Adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['categorical_accuracy'])\n",
    "\n",
    "# comment the code below if you dont use tensorboard\n",
    "index = 0\n",
    "\n",
    "while (True):\n",
    "    if not os.path.exists('logs/' + str(index)):\n",
    "        os.makedirs('logs/' + str(index))\n",
    "        break ;\n",
    "    index += 1\n",
    "    \n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs/' + str(index), \n",
    "                                          histogram_freq=0, \n",
    "                                          batch_size=batch_size, \n",
    "                                          write_graph=True, \n",
    "                                          write_grads=True, \n",
    "                                          write_images=True, \n",
    "                                          embeddings_freq=0,\n",
    "                                          embeddings_layer_names=None, \n",
    "                                          embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a multi generator for multi-label classification\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(1,150,150), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def read_and_resize(self, filepath):\n",
    "#         print ('img2/' + filepath)\n",
    "        img = imread(filepath)\n",
    "        res = resize(img, (150, 150), preserve_range=True, mode='reflect')\n",
    "        return np.expand_dims(res, 0)\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        X = [self.read_and_resize(self.list_IDs[i])\n",
    "             for i in list_IDs_temp]\n",
    "        y = self.labels[list_IDs_temp]\n",
    "        X = np.vstack(X)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'dim': (1,150,150),\n",
    "            'batch_size': 128,\n",
    "            'n_classes': len(mlb.classes_),\n",
    "            'n_channels': 3,\n",
    "            'shuffle': True\n",
    "         }\n",
    "\n",
    "training_generator = DataGenerator(X_train, y_train, **params)\n",
    "validation_generator = DataGenerator(X_test, y_test, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator for a single gpu\n",
    "multi_model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_resize(filepath):\n",
    "    img = imread(filepath)\n",
    "    res = resize(img, (150, 150), preserve_range=True, mode='reflect')\n",
    "    return np.expand_dims(res, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_img = ''\n",
    "#convert png to jpg comment if you don't need it\n",
    "im = Image.open(path_to_img)\n",
    "rgb_im = im.convert('RGB')\n",
    "rgb_im.save('image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_and_resize('image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = multi_model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict(image)[0]\n",
    "idxs = np.argsort(proba)[::-1][:2]\n",
    "print (proba)\n",
    "print (idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=mpimg.imread('image.jpg')\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the indexes of the high confidence class labels\n",
    "for (i, j) in enumerate(idxs):\n",
    "    label = \"{}: {:.2f}%\".format(mlb.classes_[j], proba[j] * 100)\n",
    "\n",
    "for (label, p) in zip(mlb.classes_, proba):\n",
    "    if (p > 0.05):\n",
    "        print (label, p * 100)\n",
    "\n",
    "print (mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save a multi_model, save from the models \n",
    "model.save('multi_label_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
