{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import functools\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import seaborn as sns\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import *\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import urllib\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 256\n",
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 100\n",
    "steps_per_epoch = 100\n",
    "INIT_LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels contains a list of list with the tags of each images\n",
    "labels = np.array(labels)\n",
    "# [list(['women', 'shoes', 'black'])\n",
    "# list(['men', 'shoes', 'green'])...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform tags to hot encoder\n",
    "mlb = MultiLabelBinarizer()\n",
    "lbl = mlb.fit_transform(labels)\n",
    "lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print classes and number of classes\n",
    "print (mlb.classes_)\n",
    "print (len(mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_test, y_train, y_test) = train_test_split(imgs,\n",
    "                                                      lbl,\n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=[150, 150, 3])\n",
    "\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "# you need to use sigmoid instead of softmax for multi-label classification\n",
    "# https://stackoverflow.com/questions/44164749/how-does-keras-handle-multilabel-classification?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "predictions = Dense(len(mlb.classes_), activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "print(\"Number of layers : \" + str(len(model.layers)))\n",
    "\n",
    "multi_model = multi_gpu_model(model, gpus = 2)\n",
    "\n",
    "# the last 50 layers are not trainable\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in model.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# binary crossentropy is also needed for multi-label\n",
    "model.compile(optimizer = 'Adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['categorical_accuracy'])\n",
    "\n",
    "#if yopu just want to train on a single gpu, just comment the multi_model\n",
    "multi_model.compile(optimizer = 'Adam',\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['categorical_accuracy'])\n",
    "\n",
    "# comment the code down below if you dont use tensorboard\n",
    "index = 0\n",
    "\n",
    "while (True):\n",
    "    if not os.path.exists('logs/' + str(index)):\n",
    "        os.makedirs('logs/' + str(index))\n",
    "        break ;\n",
    "    index += 1\n",
    "    \n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./logs/' + str(index), \n",
    "                                          histogram_freq=0, \n",
    "                                          batch_size=batch_size, \n",
    "                                          write_graph=True, \n",
    "                                          write_grads=True, \n",
    "                                          write_images=True, \n",
    "                                          embeddings_freq=0,\n",
    "                                          embeddings_layer_names=None, \n",
    "                                          embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a multi generator for multi-label classification\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(1,150,150), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def read_and_resize(self, filepath):\n",
    "        img = imread(filepath)\n",
    "        res = resize(img, (150, 150), preserve_range=True, mode='reflect')\n",
    "        return np.expand_dims(res, 0)\n",
    "            \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        X = [self.read_and_resize(self.list_IDs[i])\n",
    "             for i in list_IDs_temp]\n",
    "        y = self.labels[list_IDs_temp]\n",
    "        X = np.vstack(X)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "            'dim': (1,150,150),\n",
    "            'batch_size': 128,\n",
    "            'n_classes': len(mlb.classes_),\n",
    "            'n_channels': 3,\n",
    "            'shuffle': True\n",
    "         }\n",
    "\n",
    "training_generator = DataGenerator(X_train, y_train, **params)\n",
    "validation_generator = DataGenerator(X_test, y_test, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator for a single gpu\n",
    "multi_model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_resize(filepath):\n",
    "    img = imread(filepath)\n",
    "    res = resize(img, (150, 150), preserve_range=True, mode='reflect')\n",
    "    return np.expand_dims(res, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_img = ''\n",
    "#convert png to jpg comment if you don't need it\n",
    "im = Image.open(path_to_img)\n",
    "rgb_im = im.convert('RGB')\n",
    "rgb_im.save('image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_and_resize('image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = multi_model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = model.predict(image)[0]\n",
    "idxs = np.argsort(proba)[::-1][:2]\n",
    "print (proba)\n",
    "print (idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=mpimg.imread('image.jpg')\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the indexes of the high confidence class labels\n",
    "for (i, j) in enumerate(idxs):\n",
    "    label = \"{}: {:.2f}%\".format(mlb.classes_[j], proba[j] * 100)\n",
    "\n",
    "for (label, p) in zip(mlb.classes_, proba):\n",
    "    if (p > 0.05):\n",
    "        print (label, p * 100)\n",
    "\n",
    "print (mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save a multi_model, save from the models \n",
    "model.save('multi_label_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
